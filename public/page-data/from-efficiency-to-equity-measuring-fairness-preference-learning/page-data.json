{"componentChunkName":"component---src-templates-publication-template-js","path":"/from-efficiency-to-equity-measuring-fairness-preference-learning/","result":{"data":{"markdownRemark":{"html":"","frontmatter":{"title":"From Efficiency to Equity: Measuring Fairness in Preference Learning","date":"2024-12-12","thumbnail":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMABYYQAAWGEAHSMUDCAAAB9klEQVR42o1SiW7UMBTc//8ukAChIliVbpMSmjhJs7k3m8vOMbxxCOoKBIz0Yjv2m3fNAYJpmhDHMaIogusp+CpGGCp7VkrZuyRJkGUZiqK4sTzPUZQF0pfEng/zPGMcR1wuFyEJ4WUVkrpBJPudUGuNPfAtVvtdJuGouo2wrmu7IbIsRVUotNcCLxKRmdHKskTTNDDG4DcsK4a0hmkv9p0lZNosx/M8dFKalmz9ILAZBrJWVYVhGDBJNSQwdYtrmqN8cuB/+Qj34T3OmS+E1UZIhy3DDH0rjnqSLK/o+h6tEJl5wij7SgVIT/dwju/w5NzBTzwUXQ29Ltbfltx1Ha7ivBOu6wpe03Q/4BpLlqcjQveEb+5nxHUKjQV/giXkMFgypxj4AXozQlVnfHePUs4HabYE0ebXCGb5mGnBaGZrWvZlqzfbh8KGE5e6wnOU4tObt3j+6si0NZLGSFYDoqJFmIvJGhXdZmUn5w55M0pb1i1DfkjYS4/OaYYH514aHoACYfRJHjKrFf+GJWTPCA7GD0TM6hGjSGASiRgtkY2WVYsWR6vXYeSq7X43be+1ncHhdYS2baHCSETegK2gqBmIFVD07DOHSG1SThwmSRzHse8prRvCZVnszx2MuldA0dKZsJn+fEefXXbEAf8JEjAzVvE3/ADQa+Xzz+joWwAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/041ac8198f08cad2bda0baf959bef39a/d98b0/from_efficiency_to_equity.png","srcSet":"/static/041ac8198f08cad2bda0baf959bef39a/7ce94/from_efficiency_to_equity.png 145w,\n/static/041ac8198f08cad2bda0baf959bef39a/f246e/from_efficiency_to_equity.png 290w,\n/static/041ac8198f08cad2bda0baf959bef39a/d98b0/from_efficiency_to_equity.png 580w","sizes":"(min-width: 580px) 580px, 100vw"},"sources":[{"srcSet":"/static/041ac8198f08cad2bda0baf959bef39a/aca5d/from_efficiency_to_equity.avif 145w,\n/static/041ac8198f08cad2bda0baf959bef39a/f1259/from_efficiency_to_equity.avif 290w,\n/static/041ac8198f08cad2bda0baf959bef39a/79023/from_efficiency_to_equity.avif 580w","type":"image/avif","sizes":"(min-width: 580px) 580px, 100vw"},{"srcSet":"/static/041ac8198f08cad2bda0baf959bef39a/b7a3a/from_efficiency_to_equity.webp 145w,\n/static/041ac8198f08cad2bda0baf959bef39a/2414a/from_efficiency_to_equity.webp 290w,\n/static/041ac8198f08cad2bda0baf959bef39a/66e50/from_efficiency_to_equity.webp 580w","type":"image/webp","sizes":"(min-width: 580px) 580px, 100vw"}]},"width":800,"height":536.551724137931}}},"description":"A framework for evaluating and improving epistemic fairness in preference learning models, integrating economic theories of inequality and justice.","author":"Shreeyash Gowaikar, Hugo Berard, Rashid Mushkani, Shin Koseki","link":"https://arxiv.org/abs/2410.18841","abstract":"As AI systems, particularly generative models, increasingly influence decision-making, ensuring that they are able to fairly represent diverse human preferences becomes crucial. This paper introduces a novel framework for evaluating epistemic fairness in preference learning models inspired by economic theories of inequality and Rawlsian justice. We propose metrics adapted from the Gini Coefficient, Atkinson Index, and Kuznets Ratio to quantify fairness in these models. We validate our approach using two datasets: a custom visual preference dataset (AI-EDI-Space) and the Jester Jokes dataset. Our analysis reveals variations in model performance across users, highlighting potential epistemic injustices. We explore pre-processing and in-processing techniques to mitigate these inequalities, demonstrating a complex relationship between model efficiency and fairness. This work contributes to AI ethics by providing a framework for evaluating and improving epistemic fairness in preference learning models, offering insights for developing more inclusive AI systems in contexts where diverse human preferences are crucial.","pdf":"https://arxiv.org/pdf/2410.18841.pdf","path":"from-efficiency-to-equity-measuring-fairness-preference-learning"}}},"pageContext":{"slug":"from-efficiency-to-equity-measuring-fairness-preference-learning"}},"staticQueryHashes":["1719329430","2402622801","3000541721"],"slicesMap":{}}